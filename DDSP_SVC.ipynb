{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Original Repo：https://github.com/yxlllc/DDSP-SVC**"
      ],
      "metadata": {
        "id": "BucKkxL-GBYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training speed of a SVC project, very suitable for the free version, within the limit you can get a training degree can be used for the model file, while inference on the quality of the input source is very low, the quality of the data requirements between diffsvc and sovits, higher than sovits.\n",
        "\n",
        "The Colab part of the code is borrowed from sovits' Colab."
      ],
      "metadata": {
        "id": "q1nt6Zl3GH2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you have no special needs, the default option for this notebook is optimal.**"
      ],
      "metadata": {
        "id": "Fgq3qqRreZ4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the data in advance.\n",
        "\n",
        "Requirements **mono pure human voice**, body whispering sound / gas sound is not suitable (difficult to extract F0), it is best to remove.\n",
        "\n",
        "Processing split into 2s-10s, maybe more than 20s are possible, this project is very not eaten memory, but 2s below is not allowed. **It is recommended that about 1000 segments of data is sufficient, more may not be better.**\n",
        "\n",
        "Remember to resample to 44.1kHz when processing, non-sampling rate can run but will greatly reduce the efficiency. Remember to match the loudness.\n",
        "\n",
        "**Use Adobe Audition's™ Loudness Matching feature to resample and modify the channel and loudness matching in one go.**\n",
        "\n",
        "Because of the free version limit, it is best to pre-process locally and not to use non-44.1kHz data, which can be trained well in one limit time with average data quality.\n",
        "\n",
        "If your dataset is not of high quality, please set 'f0_extractor' to 'crepe' in the configuration file. crepe algorithm has the best noise immunity, but at the cost of greatly increasing the time needed for data preprocessing."
      ],
      "metadata": {
        "id": "2_lQDUmDA-me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset file structure:\n",
        "\n",
        "Place all the training set data (.wav format audio slices) in data/train/audio\n",
        "\n",
        "Put all the validation set data (.wav format audio slices) into data/val/audio\n",
        "\n",
        "**It is recommended to use draw.py for the validation set extraction.**\n",
        "\n",
        "Pack the data folder into a zip format named data.zip and upload it to the root directory of Google Cloud Drive.\n",
        "\n",
        "It is recommended to pre-process locally to save time on the limit, after pre-processing the same method as above can be packed and uploaded."
      ],
      "metadata": {
        "id": "7sdhuA_GaC9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two training methods are provided, \"combsub-based model (combsub)\" and \"sinusoidal additive synthesizer-based model (sin)\", the latter is less comprehensive than the former, but still provides options.\n",
        "\n",
        "**2.0 updates combsub, if you want to continue training the previous model please use the combsub-old.yaml configuration file.**"
      ],
      "metadata": {
        "id": "WL8x30ZODOZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the hyperparameters (such as \"bs\") to increase the memory occupation may not improve efficiency (I tried to reduce anyway), the default parameter is about 8.2batch / s, almost no overfitting."
      ],
      "metadata": {
        "id": "Bl61DX8OENju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/6 added the code for downloading the bottom die, the test convergence speed is increased by about 40%, and there is almost no tone leakage at 50k, so it is still possible to try."
      ],
      "metadata": {
        "id": "EBtSagh0UmUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/8 updated with multi-speaker training, the dataset structure is as follows, single person training can still use the previous dataset with no effect. Multi-speaker can be enabled by modifying the 'n_spk' option in the configuration file.\n",
        "\n",
        "```\n",
        "# Training set\n",
        "# 1st speaker\n",
        "data/train/audio/1/aaa.wav\n",
        "data/train/audio/1/bbb.wav\n",
        "...\n",
        "# 2 speaker\n",
        "data/train/audio/2/ccc.wav\n",
        "data/train/audio/2/ddd.wav\n",
        "...\n",
        "\n",
        "# Validation set\n",
        "# 1st speaker\n",
        "data/val/audio/1/eee.wav\n",
        "data/val/audio/1/fff.wav\n",
        "...\n",
        "# 2nd speaker\n",
        "data/val/audio/2/ggg.wav\n",
        "data/val/audio/2/hhh.wav\n",
        "...\n",
        "```"
      ],
      "metadata": {
        "id": "OgATNlz5KKi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/21 before PR a small script to draw cards, lazy to organize their own verification set can use this."
      ],
      "metadata": {
        "id": "5sFzAi87bL8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated 3/22 with 2.0 related and additional text."
      ],
      "metadata": {
        "id": "HmvoId2aNp_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SswdL0i0W8x",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title See what card was drawn ~~ basically T4 ~~\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVBUR74k0-tx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Clone github repository\n",
        "!git clone https://github.com/yxlllc/DDSP-SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDk-Clmv1L2w",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "%cd /content/DDSP-SVC\n",
        "!pip install pyworld praat-parselmouth torchcrepe einops local_attention wave"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two vocoder options are provided, the community-contributed vocoder trained by the OpenVPI team and the fishaudio-trained vocoder. **For DDSP-SVC, you don't have much reason to use the latter because of the presence of eak.**"
      ],
      "metadata": {
        "id": "V_vTgcz6Rd-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSt6D7SS1WyW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download the necessary files\n",
        "#@markdown ##Select vocoder\n",
        "way = \"OpenVPI\" #@param [\"OpenVPI\",\"fishaudio\"]\n",
        "\n",
        "!wget -P pretrain/hubert/ https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt\n",
        "if way == \"OpenVPI\":\n",
        "  !wget -P pretrain/ https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip\n",
        "  !unzip -d /content/DDSP-SVC/pretrain /content/DDSP-SVC/pretrain/nsf_hifigan_20221211.zip\n",
        "if way == \"fishaudio\":\n",
        "  !wget -P pretrain/ https://github.com/fishaudio/fish-diffusion/releases/download/v2.0.0/nsf_hifigan-stable-v1.zip\n",
        "  !unzip -d /content/DDSP-SVC/pretrain /content/DDSP-SVC/pretrain/nsf_hifigan-stable-v1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sFoApQR2YA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb374c4-b9cd-4b12-a1d1-381ab647b0cd",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Loading Google Cloud Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBjSKnGQ2nf9"
      },
      "source": [
        "File structure:\n",
        "\n",
        "Place all the training set data (.wav format audio slices) into data/train/audio\n",
        "\n",
        "Put all the validation set data (.wav format audio slices) into data/val/audio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get dataset from cloud drive\n",
        "\n",
        "#@markdown ###Modify the path and file name here\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "import os\n",
        "\n",
        "zip_file_path = \"\\\"/content/drive/MyDrive/data.zip\\\"\" #@param {type:\"string\"}\n",
        "!unzip $zip_file_path -d /content/tacotron2/wavs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fX04wQoZgqvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The card draw script is set based on 1000 data, please increase \"SAMPLE_MIN\" if the data is too small."
      ],
      "metadata": {
        "id": "k0Koo8iDlCV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title extract training set, already extracted can be skipped\n",
        "!python draw.py"
      ],
      "metadata": {
        "id": "v3xcldMMbPMc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MoDzyro3u7E",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title data preprocessing, already processed can be skipped\n",
        "#@markdown ##Select training method\n",
        "way = \"combsub\" #@param [\"combsub\",\"combsub-old\",\"sins\"]\n",
        "\n",
        "if way == \"combsub\":\n",
        "  !python preprocess.py -c configs/combsub.yaml\n",
        "if way == \"combsub-old\":\n",
        "  !python preprocess.py -c configs/combsub-old.yaml\n",
        "if way == \"sins\":\n",
        "  !python preprocess.py -c configs/sins.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the name of the \"dataset.zip\" file to \"data.zip\" to continue to use the dataset in the next training without the need to reprocess it."
      ],
      "metadata": {
        "id": "2C-pYqopm1KB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43HGp5n-4fgC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Pack/backup datasets\n",
        "!zip -r dataset.zip /content/DDSP-SVC/data\n",
        "!cp /content/DDSP-SVC/dataset.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0GVKPAS5dpO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title set model backup\n",
        "#@markdown **whether to backup the model to the cloud disk, colab at any time to explode the recommended backup, the default save to the cloud disk root directory DDSP-SVC folder**\n",
        "Save_to_drive = True #@param {type:\"boolean\"}\n",
        "if Save_to_drive:\n",
        "  !rm -rf /content/DDSP-SVC/exp\n",
        "  !mkdir -p /content/drive/MyDrive/DDSPSVC\n",
        "  !ln -s /content/drive/MyDrive/DDSPSVC /content/DDSP-SVC/exp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please change the 'n_spk' parameter in the configuration file to '2' when using the pre-trained model, regardless of whether you train a multi-speaker model or not.**\n",
        "\n",
        "**If you want to train a model with more speakers, don't download the pre-trained model.**\n",
        "\n",
        "~~If you need to train many multi-speaker models for a long time, maybe you can practice a base model by yourself.~~"
      ],
      "metadata": {
        "id": "GWT2E-yIZ6KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download pre-trained model (optional)\n",
        "#@markdown ## Select training method\n",
        "way = \"combsub\" #@param [\"combsub\",\"combsub-old\"]\n",
        "if way == \"combsub\":\n",
        "  !wget -P exp https://github.com/yxlllc/DDSP-SVC/releases/download/2.0/opencpop+kiritan.zip\n",
        "if way == \"combsub-old\":\n",
        "  !wget -P exp https://github.com/yxlllc/DDSP-SVC/releases/download/1.1/opencpop+kiritan.zip\n",
        "!mkdir /content/DDSP-SVC/exp/combsub-test/\n",
        "!mkdir /content/DDSP-SVC/exp/sin-test/\n",
        "!unzip -d /content/DDSP-SVC/exp /content/DDSP-SVC/exp/opencpop+kiritan.zip\n",
        "!cp /content/DDSP-SVC/exp/opencpop/model_300000.pt /content/DDSP-SVC/exp/sin-test/\n",
        "!cp /content/DDSP-SVC/exp/opencpop/model_300000.pt /content/DDSP-SVC/exp/combsub-test/"
      ],
      "metadata": {
        "id": "k95eS0A7Snc-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before training, please open the \"/DDSP-SVC/config\" folder in the \"File\" tab on the left column and change line 35 of \"combsub.yaml \" or \"sins.yaml\" (depending on your training method) on line 35, change \"cache_device: 'cpu'\" to \" cache_device: 'cuda'\", check whether the 'n_spk' parameter is correct or not, and click the \"File\" tab on the top bar to save.** This will further speed up the training process.\n",
        "\n",
        "**This will further speed up the training.** **This will further speed up the training.** \n",
        "\n",
        "**Can you increase the value of \"lr\" in line 39 in the pre-training period by using the base model? (Not tested, you need to master it by yourself, don't change it if you are not sure)**\n",
        "\n",
        "**If you have a very, very, very, very huge data set, which may explode the video memory, in this case you can choose not to modify the cache_device, but more recommended to reduce the amount of data, more data may not be good.**"
      ],
      "metadata": {
        "id": "sRhiVFhDE9GB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEbamAP96asx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Start training\n",
        "#@markdown ##Select training method\n",
        "way = \"combsub\" #@param [\"combsub\",\"sins\"]\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir exp\n",
        "\n",
        "%cd /content/DDSP-SVC\n",
        "if way == \"combsub\":\n",
        "  !python train.py -c configs/combsub.yaml\n",
        "if way == \"combsub-old\":\n",
        "  !python train.py -c configs/combsub-old.yaml\n",
        "if way == \"sins\":\n",
        "  !python train.py -c configs/sins.yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title inference (multi-speakers modify the code to specify the speaker by themselves)\n",
        "#@markdown **Upload the processed \".wav\" input source file to the cloud drive root directory and modify the following options**\n",
        "\n",
        "#@markdown **\" .wav \"File filename**\n",
        "input = \"input\" #@param {type:\"string\"}\n",
        "input_path = \"/content/drive/MyDrive/\"\n",
        "input_name =  input_path + input\n",
        "model_path = \"/content/drive/MyDrive/DDSP-SVC\"\n",
        "#@markdown **pitch adjustment**\n",
        "keychange = \"0\"  #@param {type:\"string\"}\n",
        "# 默认 enhancer_adaptive_key = 0 正常音域范围内将有更高的音质\n",
        "# 设置 enhancer_adaptive_key > 0 可将增强器适配于更高的音域\n",
        "#@markdown **EAK setting**\n",
        "enhancer_adaptive_key = \"0\"  #@param {type:\"string\"}\n",
        "!python main.py -i {input_name}.wav -m {model_path}/{way}/model_best.pt -o {input_name}_result.wav -k {keychange} -eak {enhancer_adaptive_key}"
      ],
      "metadata": {
        "id": "pMoC7hbBG7rx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}